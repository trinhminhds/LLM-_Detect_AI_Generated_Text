# LLM Detect AI-Generated Text

## Overview
In recent years, large language models (LLMs) have become increasingly sophisticated, capable of generating text that is difficult to distinguish from human-written text. In this competition, we hope to foster open research and transparency on AI detection techniques applicable in the real world.
This competition challenges participants to develop a machine learning model that can accurately detect whether an essay was written by a student or an LLM. The competition dataset comprises a mix of student-written essays and essays generated by a variety of LLMs.
This project is an AI-generated text detection system, utilizing deep learning models to classify essays as either real or fake. The project is built on the DeBERTaV3 model and Keras-NLP, with data sourced from Kaggle and external datasets.

## Description
Can you help build a model to identify which essay was written by middle and high school students, and which was written using a large language model? With the spread of LLMs, many people fear they will replace or alter work that would usually be done by humans. Educators are especially concerned about their impact on students’ skill development, though many remain optimistic that LLMs will ultimately be a useful tool to help students improve their writing skills.
At the forefront of academic concerns about LLMs is their potential to enable plagiarism. LLMs are trained on a massive dataset of text and code, which means that they are able to generate text that is very similar to human-written text. For example, students could use LLMs to generate essays that are not their own, missing crucial learning keystones. Your work on this competition can help identify telltale LLM artifacts and advance the state of the art in LLM text detection. By using texts of moderate length on a variety of subjects and multiple, unknown generative models, we aim to replicate typical detection scenarios and incentivize learning features that generalize across models.
Vanderbilt University, together with ​The Learning Agency Lab, an independent nonprofit based in Arizona, have collaborated with Kaggle on this competition.


## Installation Steps

1. **Install required libraries**:
    ```bash
    pip install -q keras_nlp==0.6.3 keras-core==0.1.7 tensorflow==2.13.0
    pip install wandb
    ```

2. **Set up the environment**:
    Make sure you are using TensorFlow with TPU or GPU. The project supports both TPU and GPU to optimize training time.

3. **Run the code**:
    After installation, you can begin running the source code to train and test the model.

## Model

The project uses the DeBERTaV3 model with a preprocessor from Keras-NLP to preprocess text and classify essays into the "real" and "fake" categories. The model is trained on a diverse dataset, including data from Kaggle and external sources.

### Key configuration parameters

- **`epochs`**: Number of training epochs (default is 3).
- **`batch_size`**: Batch size for training.
- **`sequence_length`**: Maximum sequence length (default is 200).
- **`num_classes`**: Number of classes (2 classes: "real" and "fake").
- **`device`**: Device configuration (TPU or GPU).
- **`scheduler`**: Learning rate scheduler type (default is `cosine`).

## Project Features

- **Data preprocessing**: Text data is preprocessed using the Keras-NLP DeBERTaV3 Preprocessor, including tokenization and padding.
- **Training on TPU/GPU**: The project supports training on TPU or GPU with TensorFlow, reducing training time.
- **WandB integration**: The project integrates with WandB to track training progress and store training results.

## Datasets

- **`train_essays.csv`**: Kaggle training data, including "real" and "fake" essays.
- **External datasets**: Additional datasets from sources such as `persuade_corpus` and `chat_gpt_moth`.

## Training Steps

1. **Data preprocessing**: The data is preprocessed using the Keras-NLP DeBERTaV3 Preprocessor.
2. **Training and testing data split**: Data is split into training and testing sets using cross-validation folds.
3. **Model training**: The DeBERTaV3 model is trained with the configured parameters.
4. **Tracking with WandB**: Training results are monitored and stored using WandB.


## Notes

- **System requirements**: TPU or GPU is recommended for training the model.
- **WandB support**: The project integrates with WandB to track and analyze training progress.

## Contact

- **Developer**: Ngoc Minh Trinh
